{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordered Variable Length Features\n",
    "\n",
    "If you have not already looked at the variable length features lesson please do so now, because this lesson relies heavily on that information.\n",
    "\n",
    "The difference between these lessons is that the features in our example will be ordered. So now thing of a customer with a credit card statement. That statement has an intrensic order, that is the chronological order. But that statement can also be variable length.\n",
    "\n",
    "---\n",
    "\n",
    "As before we will need to spend some time constructing an appropriate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset = make_classification(\n",
    "    n_samples=10_000, \n",
    "    n_features=30, \n",
    "    n_informative=10,\n",
    "    n_clusters_per_class=2,\n",
    "    n_classes=4)\n",
    "\n",
    "x, y = base_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "x_standardized = ss.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same trick as before to construct the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classes = []\n",
    "\n",
    "for i in range(4):\n",
    "    base_classes.append(x_standardized[y == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_points = 5_000\n",
    "class1_dist = np.array([.5, .5, 0, 0])\n",
    "class2_dist = np.array([0, .2, .6, .2])\n",
    "\n",
    "def make_var_len_feature_point(dist):\n",
    "    sequence_dist = dist.copy()\n",
    "    \n",
    "    feature_sets = []\n",
    "    previous_feature_set = np.zeros((1, 30))\n",
    "    num_features = np.random.randint(3, 11)\n",
    "    for i in range(num_features):\n",
    "        # choose which distribution the transaction comes from\n",
    "        base_class = np.random.choice([0, 1, 2, 3], 1, p=sequence_dist)\n",
    "        base_class_points = base_classes[base_class[0]]\n",
    "        feature_set_idx = np.random.choice(base_class_points.shape[0], 1)\n",
    "        previous_feature_set += base_class_points[feature_set_idx]\n",
    "        feature_sets.append(previous_feature_set)\n",
    "        \n",
    "        # now make it more likely to come from the same dist\n",
    "        dist_update = np.zeros([4]); dist_update[base_class] = 1\n",
    "        sequence_dist += dist_update\n",
    "        sequence_dist = sequence_dist / sequence_dist.sum()\n",
    "\n",
    "        \n",
    "    for _ in range(10 - num_features):\n",
    "        feature_sets.append(np.zeros((1, 30)))\n",
    "\n",
    "    return np.concatenate(feature_sets)[np.newaxis, :, :]\n",
    "\n",
    "\n",
    "class1_points = []\n",
    "for _ in range(num_points):\n",
    "    class1_points.append(\n",
    "        make_var_len_feature_point(class1_dist))\n",
    "class1_points = np.concatenate(class1_points)\n",
    "    \n",
    "class2_points = []\n",
    "for _ in range(num_points):\n",
    "    class2_points.append(\n",
    "        make_var_len_feature_point(class2_dist))\n",
    "class2_points = np.concatenate(class2_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10, 30)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2_points.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So notice the one difference from the above. The distribution of the points changes depending on the previous point. For a concrete example, you might have more transactions of a particular type if you have had a lot of that type before.\n",
    "\n",
    "Once again we pad the input as well so that we can batch them all together.\n",
    "\n",
    "---\n",
    "\n",
    "Okay, now that we have the data, let's do the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample_generator(batch_size):\n",
    "    while True:\n",
    "        batch_idx = np.random.choice(\n",
    "            class1_points.shape[0], batch_size // 2)\n",
    "        batch_x = np.concatenate([\n",
    "            class1_points[batch_idx],\n",
    "            class2_points[batch_idx],\n",
    "        ])\n",
    "        batch_y = np.concatenate([\n",
    "            np.zeros(batch_size // 2),\n",
    "            np.ones(batch_size // 2),\n",
    "        ])\n",
    "        yield ({'numeric_inputs': batch_x}, \n",
    "               {'output': batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "p = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input((10, 30), name='numeric_inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are doing a different task. We want to look at each individual input in order, consider the information, and then use that to judge the next ones. \n",
    "\n",
    "The tool that we use to do this is an RNN, in particular a GRU (gated recurrent unit). We wrap that unit in Bidirectional so that we can read from both ends. Probably less effective in the particular case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Dropout(p)(inputs)\n",
    "x = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.GRU(10))(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with before we can add global context back into the inputs and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus\n",
    "x = tf.keras.layers.RepeatVector(10)(x)\n",
    "x = tf.keras.layers.Concatenate()([inputs, x])\n",
    "\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.GRU(10))(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the rest is the good old network from the past:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Dense(20, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(inputs=inputs, outputs=out)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "numeric_inputs (InputLayer)     [(None, 10, 30)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 10, 30)       0           numeric_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 20)           2520        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_3 (Batch (None, 20)           80          bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 10, 20)       0           batch_normalization_v2_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 10, 50)       0           numeric_inputs[0][0]             \n",
      "                                                                 repeat_vector_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 10, 50)       0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 20)           3720        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_4 (Batch (None, 20)           80          bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 20)           0           batch_normalization_v2_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          2100        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_5 (Batch (None, 100)          400         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 100)          0           batch_normalization_v2_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 20)           2020        dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_6 (Batch (None, 20)           80          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 20)           0           batch_normalization_v2_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           210         dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_7 (Batch (None, 10)           40          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 10)           0           batch_normalization_v2_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            11          dropout_13[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 11,261\n",
      "Trainable params: 10,921\n",
      "Non-trainable params: 340\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "312/312 [==============================] - 15s 47ms/step - loss: 0.3919 - accuracy: 0.8189\n",
      "Epoch 2/5\n",
      "312/312 [==============================] - 5s 15ms/step - loss: 0.2500 - accuracy: 0.8928\n",
      "Epoch 3/5\n",
      "312/312 [==============================] - 5s 15ms/step - loss: 0.2192 - accuracy: 0.9104\n",
      "Epoch 4/5\n",
      "312/312 [==============================] - 5s 15ms/step - loss: 0.2186 - accuracy: 0.9076\n",
      "Epoch 5/5\n",
      "312/312 [==============================] - 5s 15ms/step - loss: 0.2018 - accuracy: 0.9148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13e762940>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model.fit_generator(\n",
    "    bootstrap_sample_generator(batch_size),\n",
    "    steps_per_epoch=10_000 // batch_size,\n",
    "    epochs=5,\n",
    "    max_queue_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this is only the tip of the iceberg for things that you could do with data like this.\n",
    "\n",
    "Next up we will go through a real world example to give us more intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
