{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Length Features\n",
    "\n",
    "Now we start to get into the stuff that NNs shine at. \n",
    "\n",
    "So we are still focusing on typical datasets, so no NL or images etc. But this time we are adding one more caveat, we can have variable length features. \n",
    "\n",
    "One example of this is trying to classify whether somebody will default on their loan given all of the credit cards that they have. \n",
    "\n",
    "Before what you'd have to do is look at aggregations of those features like: average balance of all the credit cards, max balance, etc.\n",
    "\n",
    "Now with NNs we can use all of those features directly.\n",
    "\n",
    "---\n",
    "\n",
    "To practice with this data we will need to do some work create it. We will start by using some more advanced features from the make classification function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset = make_classification(\n",
    "    n_samples=10_000, \n",
    "    n_features=30, \n",
    "    n_informative=10,\n",
    "    n_clusters_per_class=2,\n",
    "    n_classes=4)\n",
    "\n",
    "x, y = base_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this time we have four classes. We will use those to create two classes below. But before that we will normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "x_standardized = ss.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classes = []\n",
    "\n",
    "for i in range(4):\n",
    "    base_classes.append(x_standardized[y == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_points = 5_000\n",
    "class1_dist = [.5, .5, 0, 0]\n",
    "class2_dist = [0, .2, .6, .2]\n",
    "\n",
    "def make_var_len_feature_point(dist):\n",
    "    feature_sets = []\n",
    "    num_features = np.random.randint(3, 11)\n",
    "    for _ in range(num_features):\n",
    "        # choose which distribution the credit card comes from\n",
    "        base_class = np.random.choice([0, 1, 2, 3], 1, p=dist)\n",
    "        base_class_points = base_classes[base_class[0]]\n",
    "        feature_set_idx = np.random.choice(base_class_points.shape[0], 1)\n",
    "        feature_sets.append(base_class_points[feature_set_idx])\n",
    "        \n",
    "    for _ in range(10 - num_features):\n",
    "        feature_sets.append(np.zeros((1, 30)))\n",
    "\n",
    "    return np.concatenate(feature_sets)[np.newaxis, :, :]\n",
    "\n",
    "\n",
    "class1_points = []\n",
    "for _ in range(num_points):\n",
    "    class1_points.append(\n",
    "        make_var_len_feature_point(class1_dist))\n",
    "class1_points = np.concatenate(class1_points)\n",
    "    \n",
    "class2_points = []\n",
    "for _ in range(num_points):\n",
    "    class2_points.append(\n",
    "        make_var_len_feature_point(class2_dist))\n",
    "class2_points = np.concatenate(class2_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2_points.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have two classes above and that they have a variable number of feature sets (or in concrete terms, our customers have a variable number of credit cards). Each feature set represents information about a single credit card (thus they are a series of numbers).\n",
    "\n",
    "I'm making the classes/customers in class 1 and 0 distinct by saying that the credit cards they generally have are distinct. Thus those two class distributions above signify that they generally have different types of credit cards.\n",
    "\n",
    "The final thing to notice here is that we go ahead and pad people that don't have 10 cards at least up to 10. Unfortunately this is necessary if you want to have batch sizes greater than 1. That being said, in more sophisticated applications, you will see people group customers with similar number of cards together and run on batches of the same size.\n",
    "\n",
    "---\n",
    "\n",
    "Ultimately we end up with data that that consists of customers coming from different classes that have different credit cards. \n",
    "\n",
    "The next step is to make the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample_generator(batch_size):\n",
    "    while True:\n",
    "        batch_idx = np.random.choice(\n",
    "            class1_points.shape[0], batch_size // 2)\n",
    "        batch_x = np.concatenate([\n",
    "            class1_points[batch_idx],\n",
    "            class2_points[batch_idx],\n",
    "        ])\n",
    "        batch_y = np.concatenate([\n",
    "            np.zeros(batch_size // 2),\n",
    "            np.ones(batch_size // 2),\n",
    "        ])\n",
    "        yield ({'numeric_inputs': batch_x}, \n",
    "               {'output': batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "p = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we are back to just having one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input((10, 30), name='numeric_inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the big difference lay. We want to operate on a variable number of inputs. So sometimes there are 4 cards and sometimes 10. Even moreso, there is no order to these inputs.\n",
    "\n",
    "It would be nice if we could process each card separately and then combine the information about all the cards together.\n",
    "\n",
    "And we can do that with two layers:\n",
    "\n",
    "1. Conv1D: we use a convolution layer to apply the same operation to each feature set, thus processing each card separately\n",
    "2. GlogalMax/AveragePool: We use this layer to combine information from all the cards together into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Dropout(p)(inputs)\n",
    "# notice I use a kernel size of 1\n",
    "# this is because there is no information given by adjacency\n",
    "x = tf.keras.layers.Conv1D(10, 1)(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "global_ave = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "global_max = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "x = tf.keras.layers.Concatenate()([global_ave, global_max])\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we still use batch norm and dropout like before. This time though the work is done in the convolution and the pooling layers\n",
    "\n",
    "---\n",
    "\n",
    "The next step is a bit of a bonus, but I think it is a cool addition. The one problem with the above is that we consider each card separately. So one technique that has been highly effective is adding in global information to the original inputs.\n",
    "\n",
    "The way I think about this is: let's first consider all the the credit cards separately and combine that information, then let's re-examine them all in light of that information.\n",
    "\n",
    "We do this by adding that global information back onto the original inputs and then repeating the same operations we did above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus\n",
    "x = tf.keras.layers.RepeatVector(10)(x)\n",
    "x = tf.keras.layers.Concatenate()([inputs, x])\n",
    "\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Conv1D(10, 1)(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "global_ave = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "global_max = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "x = tf.keras.layers.Concatenate()([global_ave, global_max])\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have gathered all this information about the credit cards, we will feed it though the same old network we had before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Dense(20, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(inputs=inputs, outputs=out)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "numeric_inputs (InputLayer)     [(None, 10, 30)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 10, 30)       0           numeric_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 10, 10)       310         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 10, 10)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 10)           0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 10)           0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 20)           0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2 (BatchNo (None, 20)           80          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 10, 20)       0           batch_normalization_v2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 50)       0           numeric_inputs[0][0]             \n",
      "                                                                 repeat_vector[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 10, 50)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 10, 10)       510         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 10, 10)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 10)           0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 10)           0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 20)           0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_1 (Batch (None, 20)           80          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20)           0           batch_normalization_v2_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          2100        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_2 (Batch (None, 100)          400         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 100)          0           batch_normalization_v2_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           2020        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_3 (Batch (None, 20)           80          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20)           0           batch_normalization_v2_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           210         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_4 (Batch (None, 10)           40          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 10)           0           batch_normalization_v2_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            11          dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,841\n",
      "Trainable params: 5,501\n",
      "Non-trainable params: 340\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "312/312 [==============================] - 6s 19ms/step - loss: 0.5587 - accuracy: 0.7149\n",
      "Epoch 2/5\n",
      "312/312 [==============================] - 2s 7ms/step - loss: 0.3034 - accuracy: 0.8735\n",
      "Epoch 3/5\n",
      "312/312 [==============================] - 2s 6ms/step - loss: 0.2233 - accuracy: 0.9131\n",
      "Epoch 4/5\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 0.1833 - accuracy: 0.9295\n",
      "Epoch 5/5\n",
      "312/312 [==============================] - 2s 6ms/step - loss: 0.1676 - accuracy: 0.9359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13726a4a8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model.fit_generator(\n",
    "    bootstrap_sample_generator(batch_size),\n",
    "    steps_per_epoch=10_000 // batch_size,\n",
    "    epochs=5,\n",
    "    max_queue_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next lesson will be pretty similar to this one, but we will be working with ordered data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
