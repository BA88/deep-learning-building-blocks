{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whats Cooking\n",
    "\n",
    "Today we are going to be using the kaggle [What's Cooking Dataset](https://www.kaggle.com/c/whats-cooking-kernels-only/data). (Please download and load the data in appropriately to follow along below).\n",
    "\n",
    "\n",
    "This is basically a list of recipes, and we need to decide which cuisine it comes from. We can check out some of the data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingredientsFlat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "      <td>romaine lettuce black olives grape tomatoes ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "      <td>plain flour ground pepper salt tomatoes ground...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "      <td>eggs pepper salt mayonaise cooking oil green c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "      <td>water vegetable oil wheat salt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "      <td>black pepper shallots cornflour cayenne pepper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients  \\\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...   \n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...   \n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...   \n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]   \n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe...   \n",
       "\n",
       "                                     ingredientsFlat  \n",
       "0  romaine lettuce black olives grape tomatoes ga...  \n",
       "1  plain flour ground pepper salt tomatoes ground...  \n",
       "2  eggs pepper salt mayonaise cooking oil green c...  \n",
       "3                     water vegetable oil wheat salt  \n",
       "4  black pepper shallots cornflour cayenne pepper...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "recipeRaw = pd.read_json(\"./whats-cooking/train.json\")\n",
    "recipeRaw[\"ingredientsFlat\"] = recipeRaw[\"ingredients\"].apply(lambda x: ' '.join(x))\n",
    "recipeRaw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our goal is the predict the cuisine - this means a multiclassification problem. We can see all the classes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino',\n",
       "       'french', 'greek', 'indian', 'irish', 'italian', 'jamaican',\n",
       "       'japanese', 'korean', 'mexican', 'moroccan', 'russian',\n",
       "       'southern_us', 'spanish', 'thai', 'vietnamese'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(recipeRaw[\"cuisine\"].values)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For keras to be able to work with this, we will need to convert these strings into one-hot encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = recipeRaw[\"ingredientsFlat\"].values\n",
    "labels_enc = le.transform(recipeRaw[\"cuisine\"].values)\n",
    "labels = tf.keras.utils.to_categorical(labels_enc)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful numeric feature we could use, is the number of ingredients in each recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipeRaw['ingredients_len'] = recipeRaw['ingredients'].apply(len)\n",
    "doc_lengths = recipeRaw[['ingredients_len']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tucker/Desktop/deep-learning-building-blocks/env/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/tucker/Desktop/deep-learning-building-blocks/env/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "doc_lengths_standardized = ss.fit_transform(doc_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to transform the ingredients into categories. In one sense this is a pretty typical NLP problem, but the cool thing about it is that the order of the ingredients does not matter, so this is an unordered variable length features problem with high cardinality categorical variables.\n",
    "\n",
    "---\n",
    "\n",
    "To transform these into categories we use the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3065"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences\n",
    "\n",
    "t = tf.keras.preprocessing.text.Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# label encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "\n",
    "# pad documents to a max length of 40 words\n",
    "max_length = 40\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are ready for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample_generator(batch_size):\n",
    "    while True:\n",
    "        batch_idx = np.random.choice(\n",
    "            padded_docs.shape[0], batch_size)\n",
    "        yield ({'cat_inputs': padded_docs[batch_idx],\n",
    "                'numeric_inputs': doc_lengths[batch_idx]\n",
    "               }, \n",
    "               {'output': labels[batch_idx] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_sz_rule(n_cat): \n",
    "    return min(600, round(1.6 * n_cat**0.56))\n",
    "\n",
    "p = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that again we have two types of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_inputs = tf.keras.layers.Input((40,), name='cat_inputs')\n",
    "numeric_inputs = tf.keras.layers.Input((1,), name='numeric_inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we use the same rules as last time to make and add in the embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    vocab_size, \n",
    "    emb_sz_rule(vocab_size), \n",
    "    input_length=40)\n",
    "cat_x = embedding_layer(cat_inputs)\n",
    "\n",
    "global_ave = tf.keras.layers.GlobalAveragePooling1D()(cat_x)\n",
    "global_max = tf.keras.layers.GlobalMaxPool1D()(cat_x)\n",
    "x = tf.keras.layers.Concatenate()([global_ave, global_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus\n",
    "x = tf.keras.layers.RepeatVector(40)(x)\n",
    "x = tf.keras.layers.Concatenate()([cat_x, x])\n",
    "\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Conv1D(20, 1)(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "global_ave = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "global_max = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "x = tf.keras.layers.Concatenate()([global_ave, global_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then after we process the variable length data, we will add on the fixed numeric inputs (notice they go in right where they went in the first two lessons):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Concatenate()([x, numeric_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Dense(20, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "out = tf.keras.layers.Dense(20, activation='softmax', name='output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(inputs=[cat_inputs, numeric_inputs], outputs=out)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "cat_inputs (InputLayer)         [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 40, 143)      438295      cat_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 143)          0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 143)          0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 286)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_3 (RepeatVector)  (None, 40, 286)      0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 40, 429)      0           embedding_6[0][0]                \n",
      "                                                                 repeat_vector_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 40, 429)      0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 40, 20)       8600        dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 40, 20)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 20)           0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 20)           0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 40)           0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "numeric_inputs (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 41)           0           concatenate_10[0][0]             \n",
      "                                                                 numeric_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 41)           0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 100)          4200        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_12 (Batc (None, 100)          400         dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 100)          0           batch_normalization_v2_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 20)           2020        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_13 (Batc (None, 20)           80          dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 20)           0           batch_normalization_v2_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 10)           210         dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_14 (Batc (None, 10)           40          dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 10)           0           batch_normalization_v2_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 20)           220         dropout_21[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 454,065\n",
      "Trainable params: 453,805\n",
      "Non-trainable params: 260\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 2.3960 - accuracy: 0.3273\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 1.7555 - accuracy: 0.5114\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 1.5519 - accuracy: 0.5614\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 1.4215 - accuracy: 0.5966\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 1.3197 - accuracy: 0.6221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13963ab38>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "model.fit_generator(\n",
    "    bootstrap_sample_generator(batch_size),\n",
    "    steps_per_epoch=10_000 // batch_size,\n",
    "    epochs=5,\n",
    "    max_queue_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as good absolute accuracy, but hey we are looking at a different dataset with a different loss metric\n",
    "\n",
    "---\n",
    "\n",
    "I hope you can start to see how you can transform your old techniques into deep learning ones. There are a ton more things to do of course and a couple of different things I'm thinking about including:\n",
    "\n",
    "* Time series\n",
    "* Natural data (images, language, sound, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
