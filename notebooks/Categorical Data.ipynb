{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Data\n",
    "\n",
    "With this next dataset we start to move into deep learning territory. \n",
    "\n",
    "Now not all categorical data is better suited to deep learning, but high cardinality categorical data (aka columns with a lot of categories) is. \n",
    "\n",
    "Old ML algs can only treat each category as completely separate entities, whereas deep learning with the use of embeddings, can capture the similarities of some categories with others. The most classic version of this with word embeddings, but the same thing can be done with zipcodes.\n",
    "\n",
    "So let's get cracking by making a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_dataset = make_classification(\n",
    "    n_samples=10_000, \n",
    "    n_features=25, \n",
    "    n_informative=10,\n",
    "    n_classes=2)\n",
    "\n",
    "x, y = numeric_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = 100\n",
    "for i in range(5):\n",
    "    x[:, i] = pd.cut(x[:, i], num_categories, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([54.  , 52.  , 31.  , 41.  , 39.  , -0.3 ,  1.59,  1.12, -0.82,\n",
       "        0.3 ,  1.25,  0.67,  0.09, -1.39, -0.45,  1.73,  0.89, -0.97,\n",
       "       -2.52, -0.35, -0.06, -0.66, -2.65,  1.07, -1.3 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_numeric = x[:, 5:]\n",
    "x_cat = x[:, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5 different variables with 100 categories each. \n",
    "\n",
    "The next step is to standardize the inputs. The nice thing about categoricals is that we won't need to standardize them. We will still need to standardize the numerice ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "x_standardized = ss.fit_transform(x_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start to make our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "p = .1\n",
    "\n",
    "numeric_inputs = tf.keras.layers.Input((20,), name='numeric_inputs')\n",
    "cat_inputs = tf.keras.layers.Input((5,), name='cat_inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now our model takes two inputs, categorical and numeric. The categorical inputs are fed into an embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_sz_rule(n_cat): \n",
    "    return min(600, round(1.6 * n_cat**0.56))\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    num_categories, \n",
    "    emb_sz_rule(num_categories), \n",
    "    input_length=5)\n",
    "cats = embedding_layer(cat_inputs)\n",
    "cats = tf.keras.layers.Flatten()(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we make an embedding layer. An embedding layer uses a series of weights to represent each category and in that way learns how the categories relate. To find out how many weights we should use, we use the `emb_sz_rule`. It's a pretty good rule of thumb (comes from fast.ai).\n",
    "\n",
    "Next we pass both the embeddings and the numeric inputs into the same network we used last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Concatenate()([cats, numeric_inputs])\n",
    "\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Dense(20, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(\n",
    "    inputs=[numeric_inputs, cat_inputs], outputs=out)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "cat_inputs (InputLayer)         [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 5, 21)        2100        cat_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 105)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "numeric_inputs (InputLayer)     [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 125)          0           flatten_1[0][0]                  \n",
      "                                                                 numeric_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 125)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          12600       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_3 (Batch (None, 100)          400         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 100)          0           batch_normalization_v2_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 20)           2020        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_4 (Batch (None, 20)           80          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20)           0           batch_normalization_v2_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           210         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_5 (Batch (None, 10)           40          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 10)           0           batch_normalization_v2_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            11          dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,461\n",
      "Trainable params: 17,201\n",
      "Non-trainable params: 260\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bootstrap_sample_generator(batch_size):\n",
    "    while True:\n",
    "        batch_idx = np.random.choice(\n",
    "            x_standardized.shape[0], batch_size)\n",
    "        yield ({'numeric_inputs': x_standardized[batch_idx],\n",
    "                'cat_inputs': x_cat[batch_idx]}, \n",
    "               {'output': y[batch_idx]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/312 [==============================] - 4s 14ms/step - loss: 0.4936 - accuracy: 0.7644\n",
      "Epoch 2/5\n",
      "313/312 [==============================] - 1s 4ms/step - loss: 0.3458 - accuracy: 0.8538\n",
      "Epoch 3/5\n",
      "313/312 [==============================] - 1s 3ms/step - loss: 0.2670 - accuracy: 0.8915\n",
      "Epoch 4/5\n",
      "313/312 [==============================] - 1s 4ms/step - loss: 0.2487 - accuracy: 0.9066\n",
      "Epoch 5/5\n",
      "313/312 [==============================] - 1s 4ms/step - loss: 0.2096 - accuracy: 0.9228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x131795a90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model.fit_generator(\n",
    "    bootstrap_sample_generator(batch_size),\n",
    "    steps_per_epoch=10_000 / batch_size,\n",
    "    epochs=5,\n",
    "    max_queue_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely a lower accuracy (partly because we destroyed information by converting numbers into categories.\n",
    "\n",
    "Using embeddings can help out a ton with these sorts of problems. So if you have a dataset that for the most part is normal, but also has high cardinality categorical variables, then consider NNs.\n",
    "\n",
    "One thing more I'll say here is that initializing the embedding from another similar dataset can help a lot. For example initializing word vectors is a very common trend in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
