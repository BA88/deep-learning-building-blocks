{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Data\n",
    "\n",
    "Tabular data is the data that you most often see. It is data that you can cleanly write in a table. It has a set number of rows and columns, and for our example below, all the data is numeric.\n",
    "\n",
    "This is the one type of data that we will go over that is not necessarily suited to neural networks. Because it is so simple and so well studied, traditional ML can do quite well on it. \n",
    "\n",
    "That being said it makes a nice springboard to begin the rest of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this data we will be using sklearn `make_classification`. This will generate a dummy classification dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_classification(n_samples=10_000, n_features=20, n_classes=2)\n",
    "x, y = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.43096299, -0.76448651, -0.14603622, ...,  1.25647633,\n",
       "          0.95220237, -0.86957962],\n",
       "        [-2.19153189,  1.60405674,  2.62852704, ..., -1.63368447,\n",
       "          0.06884142, -0.67151764],\n",
       "        [ 1.25853375, -0.67072596, -0.32322832, ..., -0.73368536,\n",
       "         -0.64945835, -0.78647951],\n",
       "        ...,\n",
       "        [ 0.71661685,  1.36375015,  0.03613625, ...,  0.57719498,\n",
       "          0.97163191, -0.33473533],\n",
       "        [-1.37980976, -1.38402459,  0.84625386, ..., -0.87398062,\n",
       "          0.76094365, -0.6931395 ],\n",
       "        [ 0.20402275,  0.93395243, -2.28360233, ..., -0.30728972,\n",
       "          1.16765538, -1.37796527]]), array([1, 1, 1, ..., 1, 1, 0]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have two classes, this is binary classification, so predicting either 0 or a 1 based off of these 20 features.\n",
    "\n",
    "So now that we have the data we can just throw it into a NN right? \n",
    "\n",
    "Well not quite yet. Because a NN is basically a linear ML alg, we first need to scale all the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "standardized_x = ss.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, now we can just throw it into a NN :) \n",
    "\n",
    "Yup for this data there is not too much else to it but to build the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# dropout probability\n",
    "p = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be using keras to build our NN. Because this is tabular data we can follow a fairly simple structure of a NN:\n",
    "\n",
    "1. Standardize/Normalize\n",
    "2. (Optional) Regularize/Dropout\n",
    "3. Apply a Dense Layer\n",
    "\n",
    "Let me talk about the first and the last.\n",
    "\n",
    "Standardizing is important because of the way that NNs train by using gradient descent. If a particular layer's input is too big, then the gradients might be massive and the training process goes out of wack. \n",
    "\n",
    "The dense layer is the core of the NN and applies a non-linear transformation to the inputs allowing the NN to represent any non-linear function - or something like that. Regardless without that you couldn't learn.\n",
    "\n",
    "Dropout is a simple way of regularizing NNs. The reason I put this as optional, is that there is some debate on whether you need dropout in addition to batch normalization.\n",
    "\n",
    "Ultimately you can experiment with the amt of dropout you need in your network, and if it's none, so be it.\n",
    "\n",
    "---\n",
    "\n",
    "So all that being said below is our first NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input((20,), name='numeric_inputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Dropout(p)(inputs)\n",
    "x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Dense(20, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(p)(x)\n",
    "out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are probably a couple of questions as to the above:\n",
    "\n",
    "* Why so many layers?\n",
    "* Why so many neurons in each layer\n",
    "\n",
    "Well a good rule of thumb is that your NN can have as many params as the number of data points that you have, and the above NN has half as many, so we could probably increase the number of parameters. \n",
    "\n",
    "As for the width vs the depth of the network, well there has been a ton of results on either side of the aisle and honeslty I'm not sure what to tell you other than experimentation.\n",
    "\n",
    "Some things you might want to keep in mind are:\n",
    "\n",
    "* Skip connections seem to be pretty cool\n",
    "* Alternating small and large layers might be a thing too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(inputs=inputs, outputs=out)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "numeric_inputs (InputLayer)  [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               2100      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_3 (Ba (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_4 (Ba (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_5 (Ba (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,861\n",
      "Trainable params: 4,601\n",
      "Non-trainable params: 260\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final amendment to our data, I always like to use keras's `fit_generator` function, so I will often make a generator to feed data to the NN instead of using the default fit funtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bootstrap_sample_generator(batch_size):\n",
    "    while True:\n",
    "        batch_idx = np.random.choice(\n",
    "            standardized_x.shape[0], batch_size)\n",
    "        yield ({'numeric_inputs': standardized_x[batch_idx]}, \n",
    "               {'output': y[batch_idx]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/312 [==============================] - 1s 2ms/step - loss: 0.2852 - accuracy: 0.8886\n",
      "Epoch 2/5\n",
      "313/312 [==============================] - 1s 2ms/step - loss: 0.2176 - accuracy: 0.9125\n",
      "Epoch 3/5\n",
      "313/312 [==============================] - 1s 2ms/step - loss: 0.2343 - accuracy: 0.9028\n",
      "Epoch 4/5\n",
      "313/312 [==============================] - 1s 2ms/step - loss: 0.2105 - accuracy: 0.9113\n",
      "Epoch 5/5\n",
      "313/312 [==============================] - 1s 2ms/step - loss: 0.2131 - accuracy: 0.9135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x131dba908>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model.fit_generator(\n",
    "    bootstrap_sample_generator(batch_size),\n",
    "    steps_per_epoch=10_000 // batch_size,\n",
    "    epochs=5,\n",
    "    max_queue_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
